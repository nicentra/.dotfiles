#!/bin/env python3
import urllib.request
from multiprocessing import Process
import argparse
import io
import os
import shutil
import re
import time
import json
import http.client
import tarfile

base_url = "nhentai.net"
gallery_base_url = "i.nhentai.net"
api_url = "/api/gallery/"

concurrent_processes = 0
max_concurrent_processes = 15


def type_resolve(type):
    if type == "p":
        return ".png"
    elif type == "g":
        return ".gif"
    else:
        return ".jpg"


def dl_image(directory, media_id, index, type):
    global concurrent_processes
    extension = type_resolve(type)
    image_name = f"{index}{extension}"
    gallery_url = f"/galleries/{media_id}/{image_name}"
    try:
        while True:
            nhentai = http.client.HTTPSConnection(gallery_base_url)
            nhentai.request('GET', gallery_url)
            response = nhentai.getresponse()
            if response.status == 200:
                break
            else:
                print(
                    f"Failed to download {directory}/{image_name} trying again")
                time.sleep(5)
        full_path = f"{directory}/{image_name}"
        print(f"Downloading {full_path}")
        f = io.open(full_path, "wb")
        f.write(response.read())
        f.close()
        print(f"Downloaded {full_path}")
    except Exception as e:
        print(e)
    concurrent_processes = concurrent_processes - 1


def dl_gallery(gallery_info):
    global concurrent_processes
    image_processes = []
    folder_name = gallery_info['title']['pretty'].replace(' ', '_')
    media_id = gallery_info['media_id']
    try:
        shutil.rmtree(folder_name)
    except:
        pass
    finally:
        os.mkdir(folder_name)
    for i in range(1, len(gallery_info['images']['pages'])+1):
        t = gallery_info['images']['pages'][i-1]["t"]
        image_processes.append(
            Process(target=dl_image, args=(folder_name, media_id, i, t)))
    for p in image_processes:
        while concurrent_processes >= max_concurrent_processes:
            time.sleep(3)
        concurrent_processes = concurrent_processes + 1
        p.start()
    for p in image_processes:
        p.join()
    if args.tar:
        compress(folder_name)


def dl_gallery_info(url):
    global concurrent_processes
    match = re.match(r"https://nhentai\.net/g/(\d*)/", url)
    api_gallery_url = "/api/gallery/"+match.group(1)
    try:
        nhentai_api = http.client.HTTPSConnection(base_url)
        nhentai_api.request('GET', api_gallery_url)
        response = nhentai_api.getresponse()
        gallery_info = json.loads(response.read())
        dl_gallery(gallery_info)
    except Exception as e:
        print(e)
    concurrent_processes = concurrent_processes - 1


def compress(directory):
    try:
        os.remove(f"../{directory}.cbt")
    except:
        pass
    finally:
        print(f"Compressing {directory}")
        tar = tarfile.open(f"../{directory}.cbt", "x")
        tar.add(directory)
        tar.close()
        print("Done")


if __name__ == "__main__":
    processes = []
    parser = argparse.ArgumentParser(
        description='Download the specified galleries from nhentai.net')
    parser.add_argument('-t', '--tar', default=False, action='store_true',
                        help='Whether or not to tar the gallery into \'.cbt\' format after downloading. By default does not remove the raw gallery directory')
    parser.add_argument('urls', metavar='url', type=str, nargs='+',
                        help='Space seperated list of gallery urls')
    args = parser.parse_args()
    for url in args.urls:
        processes.append(Process(target=dl_gallery_info, args=(url,)))
    for p in processes:
        while concurrent_processes >= max_concurrent_processes:
            time.sleep(3)
        concurrent_processes = concurrent_processes + 1
        p.start()
    for p in processes:
        p.join()
